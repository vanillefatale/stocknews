import os
import time
from dotenv import load_dotenv
from src.config.sheet import load_sheet
from src.collectors.yahoo import YahooCollector
from src.collectors.newsapi import NewsAPICollector
from src.collectors.google_rss import GoogleRSSCollector
from src.config.settings import Settings

load_dotenv()
SHEET_ID = os.getenv("GOOGLE_SHEET_ID")


# âœ… ì¢…ëª© 50ê°œì”© ë¬¶ëŠ” ìœ í‹¸
def chunked(seq, size):
    for i in range(0, len(seq), size):
        yield seq[i:i + size]


# âœ… ì‹œíŠ¸ ë¡œë”©
def load_global_tickers(sheet_id):
    sheet = load_sheet(sheet_id, worksheet_name="global")
    tickers = sheet.col_values(1)[1:]  # Aì—´ (í—¤ë” ì œì™¸)
    return tickers, sheet


# âœ… ì¢…ëª©ëª… â†’ ì‹œíŠ¸ í–‰ë²ˆí˜¸ íƒìƒ‰
def get_row_for_ticker(sheet, ticker_name):
    tickers = sheet.col_values(1)
    for i, name in enumerate(tickers):
        if name.strip() == ticker_name.strip():
            return i + 1
    return None


# âœ… ë‰´ìŠ¤ ìˆ˜ì§‘ (Yahoo 3 + NewsAPI 1 + GoogleRSS 2)
def fetch_global_news(ticker):
    news_items = []

    yahoo = YahooCollector()
    yahoo_items = yahoo.fetch_news(ticker, count=3) or []
    for item in yahoo_items:
        news_items.append({
            "title": item["title"],
            "link": item["link"],
            "date": item["publish_date"],
            "snippet": item.get("snippet", "")
        })

    googlerss = GoogleRSSCollector()
    rss_items = googlerss.fetch_news(ticker, count=2) or []
    for item in rss_items:
        news_items.append({
            "title": item["title"],
            "link": item["link"],
            "date": item["publish_date"],
            "snippet": item.get("snippet", "")
        })

    newsapi = NewsAPICollector()
    newsapi_items = newsapi.fetch_news(ticker, count=1) or []
    for item in newsapi_items:
        news_items.append({
            "title": item["title"],
            "link": item["link"],
            "date": item["date"],
            "snippet": item.get("snippet", "")
        })

    return news_items


# âœ… ë°°ì¹˜ ì‹¤í–‰ í•¨ìˆ˜
def run_global_news_summary(sheet_id, batch_size=50):
    tickers, sheet = load_global_tickers(sheet_id)
    total_updated = 0

    # ì¢…ëª©ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘
    for batch in chunked(tickers, batch_size):
        batch_data = []
        row_map = {}

        for ticker in batch:
            print(f"ğŸŒ [ìˆ˜ì§‘ ì¤‘] {ticker}")
            row = get_row_for_ticker(sheet, ticker)
            if not row:
                print(f"âš ï¸ ì¢…ëª© '{ticker}' í–‰ ì°¾ê¸° ì‹¤íŒ¨ â†’ ê±´ë„ˆëœ€")
                continue

            news_items = fetch_global_news(ticker)

            values = [ticker]
            for item in news_items[:6]:  # ìµœëŒ€ 6ê°œ ë‰´ìŠ¤
                values.extend([item["title"], item["link"], item["date"]])
            while len(values) < 19:  # ë¶€ì¡±í•  ê²½ìš° ë¹ˆì¹¸ ì±„ìš°ê¸°
                values.extend(["", "", ""])

            row_map[row] = values
            total_updated += 1

        # ì¼ë°˜ ë‰´ìŠ¤ ë°ì´í„° ì—…ë°ì´íŠ¸
        batch_data = [
            {"range": f"A{row}:S{row}", "values": [row_map[row]]}
            for row in sorted(row_map.keys())
        ]

        if batch_data:
            sheet.batch_update(batch_data)
            print(f"âœ… ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(batch_data)}ê°œ")

        time.sleep(2)

    print(f"\nğŸ¯ ì „ì²´ ì™„ë£Œ: ì´ {total_updated}ê°œ ì¢…ëª© ì²˜ë¦¬ë¨")


# âœ… ì‹¤í–‰
if __name__ == "__main__":
    run_global_news_summary(SHEET_ID)
